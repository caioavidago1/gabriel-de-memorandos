import asyncio
import os
from typing import Dict, Any, Optional, List
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
import json
from facts_config import FIELD_VISIBILITY, get_relevant_fields_for_memo_type
from core.logger import get_logger
from core.extraction_schemas import get_schema_for_section

logger = get_logger(__name__)


def clean_extracted_value(field_key: str, value: Any) -> Any | None:
    """Limpa e converte valores extra√≠dos"""
    if not value or value == "null":
        return None
    
    if field_key.endswith("_mm"):
        if isinstance(value, str):
            cleaned = value.replace("M", "").replace("$", "").replace(",", "").strip()
            return float(cleaned) if cleaned and cleaned.replace(".", "").replace("-", "").isdigit() else None
        return float(value) if value else None
    
    if "_pct" in field_key or "percentage" in field_key:
        if isinstance(value, str):
            cleaned = value.replace("%", "").replace(",", ".").strip()
            return float(cleaned) if cleaned and cleaned.replace(".", "").replace("-", "").isdigit() else None
        return float(value) if value else None
    
    if "year" in field_key:
        return int(value) if str(value).isdigit() else None
    
    if "multiple" in field_key:
        if isinstance(value, str):
            cleaned = value.replace("x", "").replace("X", "").replace(",", ".").strip()
            return float(cleaned) if cleaned and cleaned.replace(".", "").replace("-", "").isdigit() else None
        return float(value) if value else None
    
    if isinstance(value, (int, float)):
        return value
    
    return str(value).strip() if value else None


class ExtractionAgent:
    """Agente especializado para extrair uma se√ß√£o espec√≠fica"""
    
    def __init__(self, llm: ChatOpenAI, section_name: str):
        self.llm = llm
        self.section_name = section_name
        self.max_retries = 3
    
    async def extract(
        self, 
        text: str, 
        memo_type: str,
        embeddings_data: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """Extrai facts de uma se√ß√£o espec√≠fica"""
        # Usar busca smart se embeddings_data tiver metadata
        if embeddings_data and "metadata" in embeddings_data:
            relevant_context = await self._get_relevant_context_smart(
                embeddings_data, 
                self.section_name
            )
        elif embeddings_data:
            relevant_context = await self._get_relevant_context(
                embeddings_data, 
                self.section_name
            )
        else:
            relevant_context = text
        
        prompt = self._load_prompt(self.section_name)
        relevant_fields = self._get_relevant_fields(memo_type)
        
        # Log economia de tokens
        total_fields_section = len(FIELD_VISIBILITY.get(self.section_name, {}))
        fields_to_extract = len(relevant_fields)
        fields_skipped = total_fields_section - fields_to_extract
        
        if fields_skipped > 0:
            logger.info(
                f"üéØ Otimiza√ß√£o para '{memo_type}' | "
                f"Se√ß√£o '{self.section_name}': "
                f"Extraindo {fields_to_extract}/{total_fields_section} campos "
                f"(economia de {fields_skipped} campos)"
            )
        
        if not relevant_fields:
            logger.warning(
                f"‚ö†Ô∏è Nenhum campo relevante para se√ß√£o '{self.section_name}' "
                f"no tipo '{memo_type}' - pulando extra√ß√£o"
            )
            return {}
        
        # Obter schema Pydantic para structured output
        try:
            schema_class = get_schema_for_section(self.section_name)
        except ValueError as e:
            # ‚ùå ERRO CR√çTICO: Schema n√£o encontrado
            logger.critical(
                f"‚ùå ERRO CR√çTICO: Schema n√£o encontrado para se√ß√£o '{self.section_name}'\n"
                f"   Detalhes: {e}\n"
                f"   A√ß√£o necess√°ria: Adicione o schema em core/extraction_schemas.py"
            )
            
            # Em modo STRICT (produ√ß√£o), falhar imediatamente
            if os.getenv("EXTRACTION_STRICT_MODE", "false").lower() == "true":
                logger.critical(f"üõë EXTRACTION_STRICT_MODE=true: Interrompendo extra√ß√£o")
                raise ValueError(
                    f"Extra√ß√£o imposs√≠vel sem schema para se√ß√£o: {self.section_name}\n"
                    f"Configure o schema em extraction_schemas.py ou desabilite EXTRACTION_STRICT_MODE"
                )
            
            # Em modo compatibilidade (dev), retornar vazio com aviso
            logger.warning(
                f"‚ö†Ô∏è  Caindo em modo degradado (qualidade reduzida)\n"
                f"   Dica: Para for√ßar erro, configure EXTRACTION_STRICT_MODE=true"
            )
            return {}  # Retornar dict vazio ao inv√©s de fallback degradado
        
        # Detectar se √© Search Fund para adicionar instru√ß√µes espec√≠ficas
        is_search_fund = "Search Fund" in memo_type or "Co-investimento" in memo_type
        
        # Para se√ß√µes de tabelas, usar TableExtractor para encontrar tabelas
        table_context = ""
        if self.section_name in ["projections_table", "returns_table"]:
            try:
                from core.table_extractor import TableExtractor
                extractor = TableExtractor()
                
                if self.section_name == "projections_table":
                    table_data = extractor.extract_projections_table(text)
                    if table_data:
                        import json
                        table_context = f"\n\nTABELAS DE PROJE√á√ïES ENCONTRADAS NO DOCUMENTO:\n{json.dumps(table_data, indent=2, ensure_ascii=False)}\n\nUse estas tabelas para preencher os campos de proje√ß√µes."
                
                elif self.section_name == "returns_table":
                    table_data = extractor.extract_returns_table(text)
                    if table_data:
                        import json
                        table_context = f"\n\nTABELAS DE RETORNOS ENCONTRADAS NO DOCUMENTO:\n{json.dumps(table_data, indent=2, ensure_ascii=False)}\n\nUse estas tabelas para preencher os campos de retornos."
            except Exception as e:
                logger.warning(f"Erro ao extrair tabelas para {self.section_name}: {e}")
        
        system_message = f"""Voc√™ √© um especialista em an√°lise de documentos financeiros de Private Equity.
Sua tarefa √© extrair informa√ß√µes estruturadas da se√ß√£o '{self.section_name}' com M√ÅXIMA PRECIS√ÉO.

REGRAS CR√çTICAS:
1. Extraia APENAS informa√ß√µes EXPLICITAMENTE mencionadas no documento
2. Use null para campos n√£o encontrados - NUNCA invente valores
3. Mantenha formata√ß√£o original de n√∫meros
4. Para percentuais: use valor decimal (ex: 15.5 para "15,5%")
5. Para valores monet√°rios: extraia apenas o n√∫mero (ex: 45.5 para "R$ 45,5M")
6. Anos: formato YYYY (ex: 2023)
7. Se houver ambiguidade, prefira null a chutar

ATEN√á√ÉO ESPECIAL PARA NOMES DE EMPRESAS (se√ß√£o identification):
- O nome da empresa pode N√ÉO ter label expl√≠cito como "Nome:" ou "Empresa:"
- Procure por nomes pr√≥prios no t√≠tulo, cabe√ßalho ou primeiras frases
- Nomes pr√≥prios que aparecem repetidamente s√£o candidatos fortes
- Exemplos: "Hero Seguros", "Bridge One Capital", "Project Phoenix"
- Se o documento menciona "a empresa" ou "o target", o nome geralmente est√° perto"""

        if is_search_fund and self.section_name == "identification":
            system_message += """

ATEN√á√ÉO CR√çTICA PARA SEARCH FUND (se√ß√£o identification):
- searcher_name: √â OBRIGAT√ìRIO extrair se mencionado. Procure por:
  * "search liderado por [NOME]"
  * "searcher [NOME]"
  * "empreendedor [NOME]"
  * Nomes pr√≥prios seguidos de "search" ou "busca"
  * Pode ser m√∫ltiplos nomes (ex: "Fernando Ponce e Eduardo Haro")
- search_start_date: √â OBRIGAT√ìRIO extrair se mencionado. Procure por:
  * "iniciou o search em", "busca iniciada em", "per√≠odo de busca"
  * Formatos: "1S2023", "Janeiro 2023", "2023", "1¬∫ semestre 2023"
- investor_nationality: √â OBRIGAT√ìRIO extrair se mencionado. Procure por:
  * "brasileiro", "mexicano", "americano", "nacionalidade", "origem"
  
N√ÉO DEIXE DE EXTRAIR esses campos se estiverem no documento!"""

        # Instru√ß√µes espec√≠ficas para se√ß√µes de tabelas
        if self.section_name == "projections_table":
            system_message += """

ATEN√á√ÉO CR√çTICA PARA PROJE√á√ïES (se√ß√£o projections_table):
- Procure por TABELAS de proje√ß√µes financeiras no documento
- Extraia dados ano a ano para cada cen√°rio (base, upside, downside)
- Cada linha da tabela deve ter: year, revenue_mm, ebitda_mm, ebitda_margin_pct
- Se houver m√∫ltiplas tabelas, identifique qual √© qual cen√°rio pelo contexto (t√≠tulo, texto pr√≥ximo)
- projections_years: Lista de todos os anos projetados
- projections_assumptions: Premissas detalhadas mencionadas para cada cen√°rio

FORMATO ESPERADO:
projections_base_case: [
  {"year": 2024, "revenue_mm": 100.0, "ebitda_mm": 35.0, "ebitda_margin_pct": 35.0},
  {"year": 2025, "revenue_mm": 120.0, "ebitda_mm": 45.0, "ebitda_margin_pct": 37.5},
  ...
]"""

        if self.section_name == "returns_table":
            system_message += """

ATEN√á√ÉO CR√çTICA PARA RETORNOS (se√ß√£o returns_table):
- Procure por TABELAS de retornos (IRR/MOIC) no documento
- Extraia retornos para cada cen√°rio (base, upside, downside)
- Se houver tabela de sensibilidade, extraia m√∫ltiplos cen√°rios variando m√∫ltiplo de sa√≠da ou ano
- returns_base_case: {irr_pct: float, moic: float, exit_year: int, exit_multiple: float}
- returns_sensitivity_table: Lista de cen√°rios com diferentes combina√ß√µes de m√∫ltiplo/ano

FORMATO ESPERADO:
returns_base_case: {"irr_pct": 39.8, "moic": 5.3, "exit_year": 2028, "exit_multiple": 6.0}
returns_sensitivity_table: [
  {"exit_year": 2028, "exit_multiple": 5.5, "irr_pct": 36.7, "moic": 4.8},
  {"exit_year": 2028, "exit_multiple": 6.0, "irr_pct": 39.8, "moic": 5.3},
  ...
]"""

        if self.section_name in ["gestor", "searcher"]:
            system_message += """

ATEN√á√ÉO CR√çTICA PARA GESTOR/SEARCHER (se√ß√£o gestor):
- searcher_name: Nome(s) completo(s) do(s) searcher(s) - pode ser m√∫ltiplos separados por v√≠rgula
- searcher_background: Forma√ß√£o acad√™mica completa e hist√≥rico profissional detalhado
- searcher_experience: Anos de experi√™ncia, empresas anteriores, cargos ocupados
- searcher_assessment: Resultados de assessment psicol√≥gico se mencionado (perfil, caracter√≠sticas)
- searcher_complementarity: Como os searchers se complementam (se dupla), divis√£o de pap√©is
- searcher_references: Refer√™ncias obtidas (ex-empregadores, mentores, validadores)
- searcher_track_record: Hist√≥rico de deals anteriores, experi√™ncias em M&A (se aplic√°vel)

PROCURE POR:
- Se√ß√µes como "2. Gestor", "Gestor", "Searchers", "Equipe"
- Texto sobre forma√ß√£o, experi√™ncia, assessment
- Compara√ß√µes com outros searchers conhecidos
- Valida√ß√µes e refer√™ncias"""

        if self.section_name == "board_cap_table":
            system_message += """

ATEN√á√ÉO CR√çTICA PARA BOARD E CAP TABLE (se√ß√£o board_cap_table):
- Procure por se√ß√µes sobre composi√ß√£o do board e estrutura de investidores
- board_members: Lista de membros do board com nome, role, background, indication_source
- cap_table: Lista de investidores com nome, tipo, contribution, percentual, pa√≠s
- governance_structure: Direitos de veto, tag-along, drag-along, composi√ß√£o do board
- board_commentary: Coment√°rios sobre qualidade do board e cap table

FORMATO ESPERADO:
board_members: [
  {"name": "Jo√£o Lima", "role": "Board Member", "background": "...", "indication_source": "Voke"},
  ...
]
cap_table: [
  {"investor_name": "Spectra", "investor_type": "Search Investor", "contribution_mm": 20.0, "contribution_pct": 22.0, "country": "Brazil"},
  ...
]

PROCURE POR:
- Se√ß√µes como "Board e Cap Table", "Board", "Composi√ß√£o do Board"
- Tabelas de investidores
- Listas de membros do board com backgrounds"""

        system_message += """

IMPORTANTE: Voc√™ DEVE retornar um objeto JSON v√°lido seguindo o schema fornecido.
Campos que voc√™ n√£o encontrar devem ser null ou omitidos."""

        user_message = f"""{prompt}

DOCUMENTO (CONTEXTO RELEVANTE - busca sem√¢ntica otimizada):
{relevant_context}
{table_context}

Extraia os dados seguindo RIGOROSAMENTE o schema estruturado."""

        for attempt in range(self.max_retries):
            try:
                messages = [
                    SystemMessage(content=system_message),
                    HumanMessage(content=user_message)
                ]
                
                # STRUCTURED OUTPUT - OpenAI valida automaticamente o schema
                # O strict=True j√° est√° configurado no ChatOpenAI (document_processor.py)
                response = await self.llm.with_structured_output(schema_class).ainvoke(messages)
                
                # Converter Pydantic model para dict
                extracted_dict = response.model_dump(exclude_none=True)
                
                logger.info(
                    f"‚úÖ Extra√ß√£o estruturada de '{self.section_name}': "
                    f"{len(extracted_dict)} campos encontrados"
                )
                
                # Aplicar limpeza adicional se necess√°rio
                cleaned = {
                    field_key: clean_extracted_value(field_key, field_value)
                    for field_key, field_value in extracted_dict.items()
                }
                
                return cleaned
                
            except Exception as e:
                logger.warning(
                    f"Tentativa {attempt + 1}/{self.max_retries} falhou para "
                    f"'{self.section_name}': {str(e)[:100]}"
                )
                if attempt == self.max_retries - 1:
                    logger.error(f"‚ùå Falha total na extra√ß√£o de {self.section_name}: {e}")
                    return {}
                await asyncio.sleep(2 ** attempt)
        
        return {}
    
    def _get_section_queries_hierarchical(self, section: str) -> Dict[str, List[str]]:
        """
        Retorna queries hier√°rquicas e espec√≠ficas para cada se√ß√£o.
        Estrutura: queries prim√°rias (mais espec√≠ficas) e secund√°rias (mais amplas).
        """
        queries_config = {
            "identification": {
                "primary": [
                    "nome oficial da empresa target companhia organiza√ß√£o",
                    "localiza√ß√£o sede cidade estado pa√≠s funda√ß√£o ano",
                ],
                "secondary": [
                    "neg√≥cio atividade setor descri√ß√£o empresa",
                    "deal contexto oportunidade relacionamento vendedor"
                ],
                "search_fund": [
                    "searcher empreendedor nome busca per√≠odo in√≠cio",
                    "nacionalidade investidor searcher origem"
                ]
            },
            "transaction_structure": {
                "primary": [
                    "valuation enterprise value EV equity value",
                    "m√∫ltiplo EV EBITDA multiple transa√ß√£o",
                    "estrutura pagamento cash seller note earnout"
                ],
                "secondary": [
                    "stake percentual participa√ß√£o adquirida",
                    "d√≠vida equity ratio financiamento estrutura"
                ]
            },
            "financials_history": {
                "primary": [
                    "receita faturamento revenue hist√≥rico crescimento CAGR",
                    "EBITDA margem lucro operacional hist√≥rico",
                    "d√≠vida net debt alavancagem leverage hist√≥rico"
                ],
                "secondary": [
                    "margem bruta gross margin convers√£o caixa",
                    "funcion√°rios employees opex despesas operacionais"
                ]
            },
            "saida": {
                "primary": [
                    "proje√ß√µes receita futura revenue exit",
                    "EBITDA projetado sa√≠da exit m√∫ltiplo",
                    "ano sa√≠da exit year estrat√©gia sa√≠da"
                ],
                "secondary": [
                    "drivers crescimento value creation",
                    "cen√°rio proje√ß√£o scenario type"
                ]
            },
            "returns": {
                "primary": [
                    "IRR TIR retorno esperado internal rate return",
                    "MOIC m√∫ltiplo dinheiro money multiple",
                    "holding period per√≠odo reten√ß√£o anos"
                ],
                "secondary": [
                    "entry multiple m√∫ltiplo entrada",
                    "retorno esperado retornos projetados"
                ]
            },
            "qualitative": {
                "primary": [
                    "modelo neg√≥cio business model opera√ß√£o",
                    "vantagens competitivas competitive advantages diferenciais",
                    "riscos principais key risks investimento"
                ],
                "secondary": [
                    "mercado concorrentes market share",
                    "pr√≥ximos passos next steps"
                ]
            },
            # Queries para Memo Completo Search Fund
            "gestor": {
                "primary": [
                    "searcher nome forma√ß√£o hist√≥rico profissional experi√™ncia",
                    "assessment psicol√≥gico perfil complementaridade dupla",
                    "refer√™ncias valida√ß√µes track record deals anteriores"
                ],
                "secondary": [
                    "empreendedor busca per√≠odo in√≠cio background",
                    "experi√™ncia empresas anteriores cargos"
                ]
            },
            "searcher": {
                "primary": [
                    "searcher nome forma√ß√£o hist√≥rico profissional experi√™ncia",
                    "assessment psicol√≥gico perfil complementaridade dupla",
                    "refer√™ncias valida√ß√µes track record deals anteriores"
                ],
                "secondary": [
                    "empreendedor busca per√≠odo in√≠cio background",
                    "experi√™ncia empresas anteriores cargos"
                ]
            },
            "projections_table": {
                "primary": [
                    "proje√ß√µes financeiras tabela cen√°rio base upside downside",
                    "receita EBITDA projetado ano a ano tabela",
                    "premissas proje√ß√µes crescimento margem drivers"
                ],
                "secondary": [
                    "financial projections table scenario",
                    "revenue EBITDA forecast year by year"
                ]
            },
            "returns_table": {
                "primary": [
                    "retornos esperados IRR MOIC cen√°rio base upside downside",
                    "tabela sensibilidade m√∫ltiplo sa√≠da timing",
                    "returns table IRR MOIC scenario sensitivity"
                ],
                "secondary": [
                    "retorno esperado waterfall distribui√ß√£o",
                    "exit scenarios multiple year"
                ]
            },
            "board_cap_table": {
                "primary": [
                    "board membros composi√ß√£o investidores cap table",
                    "board members investors contribution percentual",
                    "governan√ßa direitos veto tag-along drag-along"
                ],
                "secondary": [
                    "investidores participantes search investor gap investor",
                    "qualidade board cap table an√°lise"
                ]
            },
            "gestora": {
                "primary": [
                    "gestora GP general partner nome gestora",
                    "track record performance hist√≥rico exits",
                    "AUM assets under management capital sob gest√£o"
                ],
                "secondary": [
                    "equipe gest√£o s√≥cios principais",
                    "filosofia investimento estrat√©gia gestora"
                ]
            },
            "fundo": {
                "primary": [
                    "fundo nome target capta√ß√£o hard cap",
                    "vintage year closing primeiro closing",
                    "capital levantado captado commitment Spectra"
                ],
                "secondary": [
                    "portfolio investimentos estrat√©gia fundo",
                    "moeda currency per√≠odo investimento vida fundo"
                ]
            },
            "estrategia": {
                "primary": [
                    "tese investimento investment thesis estrat√©gia",
                    "setores foco geografia alvo",
                    "ticket m√©dio n√∫mero ativos tipo participa√ß√£o"
                ],
                "secondary": [
                    "est√°gio investimento stage aloca√ß√£o",
                    "diferencia√ß√£o estrat√©gia peers"
                ]
            },
            "spectra_context": {
                "primary": [
                    "Spectra relacionamento hist√≥rico gestora",
                    "contexto rationale oportunidade investimento Spectra",
                    "due diligence termos negociados fees governance"
                ],
                "secondary": [
                    "coinvestidores LPs outros investidores",
                    "aloca√ß√£o estrat√©gia Spectra"
                ]
            },
            "opinioes": {
                "primary": [
                    "opini√£o an√°lise recomenda√ß√£o Spectra",
                    "pontos positivos strengths preocupa√ß√µes concerns",
                    "pr√≥ximos passos next steps racional investimento"
                ],
                "secondary": [
                    "conclus√£o parecer avalia√ß√£o final",
                    "founders track record posicionamento empresa"
                ]
            }
        }
        
        return queries_config.get(section, {
            "primary": [section.replace("_", " ")],
            "secondary": []
        })
    
    async def _get_relevant_context(
        self, 
        embeddings_data: Dict, 
        section: str
    ) -> str:
        """Busca chunks mais relevantes para a se√ß√£o (m√©todo legado com queries hier√°rquicas)"""
        from core.document_processor import DocumentProcessor
        
        processor = DocumentProcessor()
        
        # Obter queries hier√°rquicas
        queries_config = self._get_section_queries_hierarchical(section)
        
        # Buscar com queries prim√°rias primeiro (mais espec√≠ficas)
        all_results = []
        seen_chunks = set()
        
        # 1. Buscar com queries prim√°rias (prioridade alta)
        for query in queries_config.get("primary", []):
            results = processor.search_relevant_chunks(query, embeddings_data, top_k=5)
            for r in results:
                chunk_id = hash(r["chunk"][:100])  # ID √∫nico baseado no in√≠cio do chunk
                if chunk_id not in seen_chunks:
                    all_results.append(r)
                    seen_chunks.add(chunk_id)
        
        # 2. Se n√£o encontrou o suficiente, buscar com queries secund√°rias
        if len(all_results) < 8:
            for query in queries_config.get("secondary", []):
                results = processor.search_relevant_chunks(query, embeddings_data, top_k=3)
                for r in results:
                    chunk_id = hash(r["chunk"][:100])
                    if chunk_id not in seen_chunks:
                        all_results.append(r)
                        seen_chunks.add(chunk_id)
        
        # 3. Para Search Fund, adicionar queries espec√≠ficas
        if section == "identification":
            for query in queries_config.get("search_fund", []):
                results = processor.search_relevant_chunks(query, embeddings_data, top_k=3)
                for r in results:
                    chunk_id = hash(r["chunk"][:100])
                    if chunk_id not in seen_chunks:
                        all_results.append(r)
                        seen_chunks.add(chunk_id)
        
        # Ordenar por score e pegar os melhores
        all_results.sort(key=lambda x: x["score"], reverse=True)
        
        # OTIMIZA√á√ÉO: Para identification, buscar MAIS chunks (incluir in√≠cio do doc)
        top_k = 15 if section == "identification" else 10
        final_results = all_results[:top_k]
        
        # Combinar chunks
        context = "\n\n".join([r["chunk"] for r in final_results])
        
        logger.info(
            f"üîç Busca hier√°rquica '{section}': "
            f"{len(final_results)} chunks √∫nicos de {len(all_results)} encontrados"
        )
        
        return context
    
    async def _get_relevant_context_smart(
        self,
        embeddings_data: Dict,
        section: str
    ) -> str:
        """
        Busca inteligente com suporte a ChromaDB OU NumPy usando queries hier√°rquicas.
        
        Detecta automaticamente se embeddings_data vem do ChromaDB ou NumPy:
        - ChromaDB: Usa query vetorial direta no index persistente
        - NumPy: Busca em mem√≥ria com metadata boost
        
        Usa queries hier√°rquicas: primeiro tenta queries prim√°rias (espec√≠ficas),
        depois expande com queries secund√°rias se necess√°rio.
        
        Args:
            embeddings_data: Dados com embeddings (ChromaDB ou NumPy)
            section: Nome da se√ß√£o a extrair
            
        Returns:
            Contexto relevante concatenado
        """
        from core.document_processor import DocumentProcessor
        processor = DocumentProcessor()
        
        # Obter queries hier√°rquicas
        queries_config = self._get_section_queries_hierarchical(section)
        
        # OTIMIZA√á√ÉO: Para identification, buscar MAIS chunks (nomes geralmente no in√≠cio)
        top_k = 15 if section == "identification" else 10
        
        # DETECTAR MODO: ChromaDB ou NumPy
        if embeddings_data.get("vector_store") == "chromadb":
            # MODO CHROMADB: Busca persistente com queries hier√°rquicas
            memo_id = embeddings_data["memo_id"]
            
            logger.info(f"üéØ Busca ChromaDB hier√°rquica para '{section}' (memo: {memo_id})")
            
            # Buscar com queries prim√°rias primeiro (mais espec√≠ficas)
            all_results = []
            seen_chunks = set()
            
            # 1. Buscar com queries prim√°rias (prioridade alta)
            for query in queries_config.get("primary", []):
                results = processor.search_chromadb_chunks(
                    memo_id=memo_id,
                    query=query,
                    top_k=5,
                    section=None
                )
                for r in results:
                    chunk_id = hash(r["chunk"][:100])  # ID √∫nico baseado no in√≠cio do chunk
                    if chunk_id not in seen_chunks:
                        all_results.append(r)
                        seen_chunks.add(chunk_id)
            
            # 2. Se n√£o encontrou o suficiente, buscar com queries secund√°rias
            if len(all_results) < 8:
                for query in queries_config.get("secondary", []):
                    results = processor.search_chromadb_chunks(
                        memo_id=memo_id,
                        query=query,
                        top_k=3,
                        section=None
                    )
                    for r in results:
                        chunk_id = hash(r["chunk"][:100])
                        if chunk_id not in seen_chunks:
                            all_results.append(r)
                            seen_chunks.add(chunk_id)
            
            # 3. Para Search Fund, adicionar queries espec√≠ficas
            if section == "identification":
                for query in queries_config.get("search_fund", []):
                    results = processor.search_chromadb_chunks(
                        memo_id=memo_id,
                        query=query,
                        top_k=3,
                        section=None
                    )
                    for r in results:
                        chunk_id = hash(r["chunk"][:100])
                        if chunk_id not in seen_chunks:
                            all_results.append(r)
                            seen_chunks.add(chunk_id)
            
            if not all_results:
                logger.warning(
                    f"‚ö†Ô∏è Busca ChromaDB hier√°rquica n√£o retornou resultados para '{section}'. "
                    f"Fallback para busca legado."
                )
                return await self._get_relevant_context(embeddings_data, section)
            
            # Ordenar por score e pegar os melhores
            all_results.sort(key=lambda x: x["score"], reverse=True)
            final_results = all_results[:top_k]
            
            # Combinar chunks
            context = "\n\n".join([r["chunk"] for r in final_results])
            
            # Log estat√≠sticas
            avg_score = sum(r["score"] for r in final_results) / len(final_results) if final_results else 0
            sections_found = set(
                r.get("metadata", {}).get("section_title", "Unknown")
                for r in final_results
            )
            
            logger.info(
                f"‚úÖ Busca ChromaDB hier√°rquica '{section}': "
                f"{len(final_results)} chunks √∫nicos (de {len(all_results)} encontrados) | "
                f"score m√©dio: {avg_score:.3f} | "
                f"se√ß√µes: {', '.join(list(sections_found)[:3])}"
            )
            
            return context
        
        else:
            # MODO NUMPY: Busca em mem√≥ria (fallback) com queries hier√°rquicas
            # Verificar se tem metadata
            if "metadata" not in embeddings_data:
                logger.warning(
                    f"‚ö†Ô∏è embeddings_data sem metadata. "
                    f"Fallback para busca legado."
                )
                return await self._get_relevant_context(embeddings_data, section)
            
            # Buscar com queries hier√°rquicas
            all_results = []
            seen_chunks = set()
            
            # 1. Buscar com queries prim√°rias (prioridade alta)
            for query in queries_config.get("primary", []):
                results = processor.search_relevant_chunks_with_metadata(
                    query=query,
                    embeddings_data=embeddings_data,
                    section_filter=None,
                    top_k=5
                )
                for r in results:
                    chunk_id = hash(r["chunk"][:100])
                    if chunk_id not in seen_chunks:
                        all_results.append(r)
                        seen_chunks.add(chunk_id)
            
            # 2. Se n√£o encontrou o suficiente, buscar com queries secund√°rias
            if len(all_results) < 8:
                for query in queries_config.get("secondary", []):
                    results = processor.search_relevant_chunks_with_metadata(
                        query=query,
                        embeddings_data=embeddings_data,
                        section_filter=None,
                        top_k=3
                    )
                    for r in results:
                        chunk_id = hash(r["chunk"][:100])
                        if chunk_id not in seen_chunks:
                            all_results.append(r)
                            seen_chunks.add(chunk_id)
            
            # 3. Para Search Fund, adicionar queries espec√≠ficas
            if section == "identification":
                for query in queries_config.get("search_fund", []):
                    results = processor.search_relevant_chunks_with_metadata(
                        query=query,
                        embeddings_data=embeddings_data,
                        section_filter=None,
                        top_k=3
                    )
                    for r in results:
                        chunk_id = hash(r["chunk"][:100])
                        if chunk_id not in seen_chunks:
                            all_results.append(r)
                            seen_chunks.add(chunk_id)
            
            if not all_results:
                logger.warning(
                    f"‚ö†Ô∏è Busca smart hier√°rquica n√£o retornou resultados para '{section}'. "
                    f"Tentando busca legado..."
                )
                return await self._get_relevant_context(embeddings_data, section)
            
            # Ordenar por score e pegar os melhores
            all_results.sort(key=lambda x: x.get("boosted_score", x["score"]), reverse=True)
            final_results = all_results[:top_k]
            
            # Log estat√≠sticas
            avg_score = sum(r["score"] for r in final_results) / len(final_results) if final_results else 0
            sections_found = set(
                r.get("metadata", {}).get("section_title", "Unknown")
                for r in final_results
            )
            
            logger.info(
                f"üéØ Busca sem√¢ntica hier√°rquica para '{section}': "
                f"{len(final_results)} chunks √∫nicos (de {len(all_results)} encontrados) | "
                f"score m√©dio: {avg_score:.3f} | "
                f"se√ß√µes encontradas: {', '.join(list(sections_found)[:3])}"
            )
        
            # Combinar chunks (menos contexto, mais preciso)
            context = "\n\n".join([r["chunk"] for r in final_results])
        
        # Log economia de tokens
        old_method_chars = 60000  # M√©todo antigo pegava ~60k chars
        new_method_chars = len(context)
        savings_pct = ((old_method_chars - new_method_chars) / old_method_chars) * 100
        
        logger.info(
            f"üí∞ Economia de tokens: {new_method_chars} chars "
            f"(vs {old_method_chars} antigo) = {savings_pct:.1f}% redu√ß√£o"
        )
        
        return context
    
    def _load_prompt(self, section: str) -> str:
        """Carrega prompt espec√≠fico da se√ß√£o"""
        prompt_file = f"facts/prompts/{section}.txt"
        try:
            with open(prompt_file, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            return f"Extraia informa√ß√µes relevantes para a se√ß√£o {section}."
    
    def _get_relevant_fields(self, memo_type: str) -> Dict:
        """
        Retorna apenas os campos relevantes para o tipo de memorando.
        
        Usa get_relevant_fields_for_memo_type() de facts_config.py para obter
        a lista filtrada de campos que devem ser extra√≠dos.
        
        Args:
            memo_type: Tipo de memorando selecionado
        
        Returns:
            Dict {field_key: field_label} com apenas campos relevantes
        """
        # Obter todos os campos relevantes para o tipo de memo
        all_relevant = get_relevant_fields_for_memo_type(memo_type)
        
        # Filtrar apenas os campos da se√ß√£o atual
        if self.section_name not in all_relevant:
            logger.warning(f"Se√ß√£o '{self.section_name}' n√£o tem campos relevantes para '{memo_type}'")
            return {}
        
        section_fields = all_relevant[self.section_name]
        
        # Criar dict com labels amig√°veis
        relevant = {}
        for field_key in section_fields:
            # Converter field_key em label (ex: "gestora_nome" ‚Üí "Gestora Nome")
            label = field_key.replace("_", " ").title()
            relevant[field_key] = label
        
        logger.info(f"Se√ß√£o '{self.section_name}' para '{memo_type}': {len(relevant)} campos relevantes")
        
        return relevant



