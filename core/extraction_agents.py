import asyncio
import os
from typing import Dict, Any, Optional, List
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
import json
from tipo_memorando.registry import get_fatos_config, get_fatos_module
from core.logger import get_logger
from core.extraction_schemas import get_schema_for_section

logger = get_logger(__name__)


def clean_extracted_value(field_key: str, value: Any) -> Any | None:
    """Limpa e converte valores extra√≠dos"""
    if not value or value == "null":
        return None

    # Preservar listas e dicts (projections_table, returns_table, board_cap_table)
    if isinstance(value, (list, dict)):
        return value

    if field_key.endswith("_mm"):
        if isinstance(value, str):
            cleaned = value.replace("M", "").replace("$", "").replace(",", "").strip()
            return float(cleaned) if cleaned and cleaned.replace(".", "").replace("-", "").isdigit() else None
        return float(value) if value else None
    
    if "_pct" in field_key or "percentage" in field_key:
        if isinstance(value, str):
            cleaned = value.replace("%", "").replace(",", ".").strip()
            return float(cleaned) if cleaned and cleaned.replace(".", "").replace("-", "").isdigit() else None
        return float(value) if value else None
    
    if "year" in field_key:
        return int(value) if str(value).isdigit() else None
    
    if "multiple" in field_key:
        if isinstance(value, str):
            cleaned = value.replace("x", "").replace("X", "").replace(",", ".").strip()
            return float(cleaned) if cleaned and cleaned.replace(".", "").replace("-", "").isdigit() else None
        return float(value) if value else None
    
    if isinstance(value, (int, float)):
        return value
    
    return str(value).strip() if value else None


class ExtractionAgent:
    """Agente especializado para extrair uma se√ß√£o espec√≠fica"""
    
    def __init__(self, llm: ChatOpenAI, section_name: str):
        self.llm = llm
        self.section_name = section_name
        self.max_retries = 3
    
    async def extract(
        self, 
        text: str, 
        memo_type: str,
        embeddings_data: Optional[Dict] = None
    ) -> Dict[str, Any]:
        """Extrai facts de uma se√ß√£o espec√≠fica"""
        # Usar busca smart se embeddings_data tiver metadata
        if embeddings_data and "metadata" in embeddings_data:
            relevant_context = await self._get_relevant_context_smart(
                embeddings_data, 
                self.section_name
            )
        elif embeddings_data:
            relevant_context = await self._get_relevant_context(
                embeddings_data, 
                self.section_name
            )
        else:
            relevant_context = text

        # Se√ß√µes que costumam estar no in√≠cio do doc: incluir capa/header quando usamos busca por chunks
        if text and embeddings_data and self.section_name in ("identification", "gestora"):
            head = text[:10000].strip()
            if head:
                relevant_context = head + "\n\n---\n\n" + (relevant_context or "")
                logger.info(f"üìÑ {self.section_name}: in√≠cio do documento (capa/header) inclu√≠do no contexto")

        # M√≥dulo de extra√ß√£o do tipo (l√≥gica completa: system message e prompt)
        fatos_module = get_fatos_module(memo_type)
        extraction = getattr(fatos_module, "extraction", None)
        self._extraction_module = extraction
        if extraction is None or not hasattr(extraction, "get_system_message") or not hasattr(extraction, "get_prompt"):
            logger.error(
                f"M√≥dulo extraction do tipo n√£o encontrado ou incompleto para '{memo_type}'. "
                f"Necess√°rio: get_system_message(section, memo_type) e get_prompt(section, memo_type)."
            )
            return {}

        config = get_fatos_config(memo_type)
        relevant_fields = self._get_relevant_fields(memo_type)
        prompt = extraction.get_prompt(self.section_name, memo_type)
        
        # Log economia de tokens
        total_fields_section = len(getattr(config, "FIELD_VISIBILITY", {}).get(self.section_name, {}))
        fields_to_extract = len(relevant_fields)
        fields_skipped = total_fields_section - fields_to_extract
        
        if fields_skipped > 0:
            logger.info(
                f"üéØ Otimiza√ß√£o para '{memo_type}' | "
                f"Se√ß√£o '{self.section_name}': "
                f"Extraindo {fields_to_extract}/{total_fields_section} campos "
                f"(economia de {fields_skipped} campos)"
            )
        
        if not relevant_fields:
            logger.warning(
                f"‚ö†Ô∏è Nenhum campo relevante para se√ß√£o '{self.section_name}' "
                f"no tipo '{memo_type}' - pulando extra√ß√£o"
            )
            return {}
        
        # Obter schema Pydantic para structured output (tipo pode sobrescrever com get_schema)
        schema_class = None
        if hasattr(extraction, "get_schema"):
            schema_class = extraction.get_schema(self.section_name, memo_type)
        if schema_class is None:
            try:
                schema_class = get_schema_for_section(self.section_name)
            except ValueError as e:
                # ‚ùå ERRO CR√çTICO: Schema n√£o encontrado
                logger.critical(
                    f"‚ùå ERRO CR√çTICO: Schema n√£o encontrado para se√ß√£o '{self.section_name}'\n"
                    f"   Detalhes: {e}\n"
                    f"   A√ß√£o necess√°ria: Adicione o schema em core/extraction_schemas.py"
                )
                if os.getenv("EXTRACTION_STRICT_MODE", "false").lower() == "true":
                    logger.critical(f"üõë EXTRACTION_STRICT_MODE=true: Interrompendo extra√ß√£o")
                    raise ValueError(
                        f"Extra√ß√£o imposs√≠vel sem schema para se√ß√£o: {self.section_name}\n"
                        f"Configure o schema em extraction_schemas.py ou desabilite EXTRACTION_STRICT_MODE"
                    )
                logger.warning(
                    f"‚ö†Ô∏è  Caindo em modo degradado (qualidade reduzida)\n"
                    f"   Dica: Para for√ßar erro, configure EXTRACTION_STRICT_MODE=true"
                )
                return {}
        
        # Para se√ß√µes de tabelas, usar TableExtractor para encontrar tabelas
        table_context = ""
        if self.section_name in ["projections_table", "returns_table"]:
            try:
                from core.table_extractor import TableExtractor
                extractor = TableExtractor()
                
                if self.section_name == "projections_table":
                    table_data = extractor.extract_projections_table(text)
                    if table_data:
                        import json
                        table_context = f"\n\nTABELAS DE PROJE√á√ïES ENCONTRADAS NO DOCUMENTO:\n{json.dumps(table_data, indent=2, ensure_ascii=False)}\n\nUse estas tabelas para preencher os campos de proje√ß√µes."
                
                elif self.section_name == "returns_table":
                    table_data = extractor.extract_returns_table(text)
                    if table_data:
                        import json
                        table_context = f"\n\nTABELAS DE RETORNOS ENCONTRADAS NO DOCUMENTO:\n{json.dumps(table_data, indent=2, ensure_ascii=False)}\n\nUse estas tabelas para preencher os campos de retornos."
            except Exception as e:
                logger.warning(f"Erro ao extrair tabelas para {self.section_name}: {e}")
        
        system_message = extraction.get_system_message(self.section_name, memo_type)

        user_message = f"""{prompt}

DOCUMENTO (CONTEXTO RELEVANTE - busca sem√¢ntica otimizada):
{relevant_context}
{table_context}

Extraia os dados seguindo RIGOROSAMENTE o schema estruturado."""

        for attempt in range(self.max_retries):
            try:
                messages = [
                    SystemMessage(content=system_message),
                    HumanMessage(content=user_message)
                ]
                
                # STRUCTURED OUTPUT - OpenAI valida automaticamente o schema
                # O strict=True j√° est√° configurado no ChatOpenAI (document_processor.py)
                response = await self.llm.with_structured_output(schema_class).ainvoke(messages)
                
                # Converter Pydantic model para dict
                extracted_dict = response.model_dump(exclude_none=True)
                
                logger.info(
                    f"‚úÖ Extra√ß√£o estruturada de '{self.section_name}': "
                    f"{len(extracted_dict)} campos encontrados"
                )
                
                # Aplicar limpeza adicional se necess√°rio
                cleaned = {
                    field_key: clean_extracted_value(field_key, field_value)
                    for field_key, field_value in extracted_dict.items()
                }
                
                return cleaned
                
            except Exception as e:
                logger.warning(
                    f"Tentativa {attempt + 1}/{self.max_retries} falhou para "
                    f"'{self.section_name}': {str(e)[:100]}"
                )
                if attempt == self.max_retries - 1:
                    logger.error(f"‚ùå Falha total na extra√ß√£o de {self.section_name}: {e}")
                    return {}
                await asyncio.sleep(2 ** attempt)
        
        return {}
    
    def _get_section_queries_hierarchical(self, section: str) -> Dict[str, List[str]]:
        """
        Retorna queries hier√°rquicas e espec√≠ficas para cada se√ß√£o.
        Estrutura: queries prim√°rias (mais espec√≠ficas) e secund√°rias (mais amplas).
        Se o tipo tiver get_section_queries(section), usa esse dict; sen√£o usa o interno.
        """
        ext = getattr(self, "_extraction_module", None)
        if ext is not None and hasattr(ext, "get_section_queries"):
            custom = ext.get_section_queries(section)
            if custom and isinstance(custom, dict):
                return custom
        queries_config = {
            "identification": {
                "primary": [
                    "empresa alvo target companhia nome codinome Project Baja TSE Disktrans Oca Hero Seguros Bridge One aquisi√ß√£o est√° avaliando",
                    "localiza√ß√£o empresa alvo cidade regi√£o pa√≠s sede S√£o Paulo Brasil Baja California M√©xico",
                    "descri√ß√£o neg√≥cio especializada em MGA seguros viagem automa√ß√£o industrial software log√≠stica",
                    "gestora nome funda√ß√£o AUM total fundo espec√≠fico ve√≠culo coinvestimento FIP SPE data oportunidade apresentado por",
                    "capa t√≠tulo in√≠cio documento MEMORANDO DE INVESTIMENTO CIM Novembro 2025 confidencial Sum√°rio Executivo"
                ],
                "secondary": [
                    "neg√≥cio atividade setor segmento core business opera√ß√£o produtos servi√ßos",
                    "contexto oportunidade deal origem relacionamento vendedor sucess√£o fundador",
                    "gestora apresenta√ß√£o AUM bilh√µes milh√µes fundo ve√≠culo data setor localiza√ß√£o"
                ],
                "search_fund": [
                    "searcher nome liderado Pedro Dorea Fernando Ponce Eduardo Haro Hunibert Tuch Guilherme Ferrari per√≠odo de busca",
                    "FIP casca ve√≠culo jur√≠dico Minerva Capital Eunoia Redfoot Entrevo Capital capta recursos investimento empresa alvo",
                    "nacionalidade search mexicano brasileiro search fund segundo semestre 2S2024 1S2023 in√≠cio per√≠odo busca",
                    "capa t√≠tulo cover deal CIM memo in√≠cio documento SEARCH FUND nome casca Atlante Atalante"
                ]
            },
            "transaction_structure": {
                "primary": [
                    "valuation enterprise value EV equity value equity value milh√µes",
                    "m√∫ltiplo EV EBITDA multiple transa√ß√£o entrada per√≠odo refer√™ncia",
                    "estrutura pagamento cash seller note earnout √† vista",
                    "valor total transa√ß√£o incluindo custos step-up search capital chega a"
                ],
                "secondary": [
                    "stake percentual participa√ß√£o adquirida 100% equity",
                    "d√≠vida equity ratio financiamento acquisition debt",
                    "estruturados percentual √† vista acquisition debt negocia√ß√£o",
                    "m√∫ltiplo total EBITDA considerando custos 4,9x 3,6x"
                ]
            },
            "financials_history": {
                "primary": [
                    "receita faturamento revenue hist√≥rico crescimento CAGR",
                    "EBITDA margem lucro operacional hist√≥rico",
                    "d√≠vida net debt alavancagem leverage hist√≥rico"
                ],
                "secondary": [
                    "margem bruta gross margin convers√£o caixa",
                    "funcion√°rios employees opex despesas operacionais"
                ]
            },
            "saida": {
                "primary": [
                    "proje√ß√µes receita futura revenue exit",
                    "EBITDA projetado sa√≠da exit m√∫ltiplo",
                    "ano sa√≠da exit year estrat√©gia sa√≠da"
                ],
                "secondary": [
                    "drivers crescimento value creation",
                    "cen√°rio proje√ß√£o scenario type"
                ]
            },
            "returns": {
                "primary": [
                    "IRR TIR retorno esperado internal rate return",
                    "MOIC m√∫ltiplo dinheiro money multiple",
                    "holding period per√≠odo reten√ß√£o anos"
                ],
                "secondary": [
                    "entry multiple m√∫ltiplo entrada",
                    "retorno esperado retornos projetados"
                ]
            },
            "qualitative": {
                "primary": [
                    "modelo neg√≥cio business model opera√ß√£o",
                    "vantagens competitivas competitive advantages diferenciais",
                    "riscos principais key risks investimento"
                ],
                "secondary": [
                    "mercado concorrentes market share",
                    "pr√≥ximos passos next steps"
                ]
            },
            # Queries para Memo Completo Search Fund
            "gestor": {
                "primary": [
                    "searcher nome forma√ß√£o hist√≥rico profissional experi√™ncia",
                    "assessment psicol√≥gico perfil complementaridade dupla",
                    "refer√™ncias valida√ß√µes track record deals anteriores"
                ],
                "secondary": [
                    "empreendedor busca per√≠odo in√≠cio background",
                    "experi√™ncia empresas anteriores cargos"
                ]
            },
            "searcher": {
                "primary": [
                    "searcher nome forma√ß√£o hist√≥rico profissional experi√™ncia",
                    "assessment psicol√≥gico perfil complementaridade dupla",
                    "refer√™ncias valida√ß√µes track record deals anteriores"
                ],
                "secondary": [
                    "empreendedor busca per√≠odo in√≠cio background",
                    "experi√™ncia empresas anteriores cargos"
                ]
            },
            "projections_table": {
                "primary": [
                    "proje√ß√µes financeiras tabela cen√°rio base upside downside",
                    "receita EBITDA projetado ano a ano tabela",
                    "premissas proje√ß√µes crescimento margem drivers"
                ],
                "secondary": [
                    "financial projections table scenario",
                    "revenue EBITDA forecast year by year"
                ]
            },
            "returns_table": {
                "primary": [
                    "retornos esperados IRR MOIC cen√°rio base upside downside",
                    "tabela sensibilidade m√∫ltiplo sa√≠da timing",
                    "returns table IRR MOIC scenario sensitivity"
                ],
                "secondary": [
                    "retorno esperado waterfall distribui√ß√£o",
                    "exit scenarios multiple year"
                ]
            },
            "board_cap_table": {
                "primary": [
                    "board membros composi√ß√£o investidores cap table",
                    "board members investors contribution percentual",
                    "governan√ßa direitos veto tag-along drag-along"
                ],
                "secondary": [
                    "investidores participantes search investor gap investor",
                    "qualidade board cap table an√°lise"
                ]
            },
            "gestora": {
                "primary": [
                    "track record fundos TVPI DPI IRR vintage performance hist√≥rica",
                    "gestora GP general partner nome s√≥cios equipe gest√£o",
                    "exits realizados principais vendas m√∫ltiplos sa√≠da empresas",
                    "AUM assets under management capital sob gest√£o fundo espec√≠fico"
                ],
                "secondary": [
                    "equipe gest√£o s√≥cios principais anos experi√™ncia background",
                    "estrat√©gia investimento tese gestora filosofia foco setorial",
                    "performance hist√≥rica IRR MOIC fundos anteriores compara√ß√£o",
                    "Spectra relacionamento anterior co-investimentos opera√ß√µes"
                ]
            },
            "fundo": {
                "primary": [
                    "fundo nome target capta√ß√£o hard cap",
                    "vintage year closing primeiro closing",
                    "capital levantado captado commitment Spectra"
                ],
                "secondary": [
                    "portfolio investimentos estrat√©gia fundo",
                    "moeda currency per√≠odo investimento vida fundo"
                ]
            },
            "estrategia": {
                "primary": [
                    "tese investimento investment thesis estrat√©gia",
                    "setores foco geografia alvo",
                    "ticket m√©dio n√∫mero ativos tipo participa√ß√£o"
                ],
                "secondary": [
                    "est√°gio investimento stage aloca√ß√£o",
                    "diferencia√ß√£o estrat√©gia peers"
                ]
            },
            "spectra_context": {
                "primary": [
                    "Spectra relacionamento hist√≥rico gestora",
                    "contexto rationale oportunidade investimento Spectra",
                    "due diligence termos negociados fees governance"
                ],
                "secondary": [
                    "coinvestidores LPs outros investidores",
                    "aloca√ß√£o estrat√©gia Spectra"
                ]
            },
            "opinioes": {
                "primary": [
                    "opini√£o an√°lise recomenda√ß√£o Spectra",
                    "pontos positivos strengths preocupa√ß√µes concerns",
                    "pr√≥ximos passos next steps racional investimento"
                ],
                "secondary": [
                    "conclus√£o parecer avalia√ß√£o final",
                    "founders track record posicionamento empresa"
                ]
            },
            "estrutura_veiculo": {
                "primary": [
                    "estrutura ve√≠culo coinvestimento regulamento fundo",
                    "dura√ß√£o fundo anos capital autorizado taxa gest√£o performance",
                    "hurdle rate catch-up prefer√™ncia distribui√ß√£o waterfall",
                    "chamadas capital chamada capital timing valores",
                    "qu√≥rum destitui√ß√£o gestor evento equipe chave"
                ],
                "secondary": [
                    "regulamento pontos aten√ß√£o governan√ßa",
                    "taxa administra√ß√£o taxa performance carry"
                ]
            },
        }
        
        return queries_config.get(section, {
            "primary": [section.replace("_", " ")],
            "secondary": []
        })
    
    async def _get_relevant_context(
        self, 
        embeddings_data: Dict, 
        section: str
    ) -> str:
        """Busca chunks mais relevantes para a se√ß√£o (m√©todo legado com queries hier√°rquicas)"""
        from core.document_processor import DocumentProcessor
        
        processor = DocumentProcessor()
        
        # Obter queries hier√°rquicas
        queries_config = self._get_section_queries_hierarchical(section)
        
        # Buscar com queries prim√°rias primeiro (mais espec√≠ficas)
        all_results = []
        seen_chunks = set()
        
        # 1. Buscar com queries prim√°rias (prioridade alta)
        for query in queries_config.get("primary", []):
            results = processor.search_relevant_chunks(query, embeddings_data, top_k=5)
            for r in results:
                chunk_id = hash(r["chunk"][:100])  # ID √∫nico baseado no in√≠cio do chunk
                if chunk_id not in seen_chunks:
                    all_results.append(r)
                    seen_chunks.add(chunk_id)
        
        # 2. Se n√£o encontrou o suficiente, buscar com queries secund√°rias
        if len(all_results) < 8:
            for query in queries_config.get("secondary", []):
                results = processor.search_relevant_chunks(query, embeddings_data, top_k=3)
                for r in results:
                    chunk_id = hash(r["chunk"][:100])
                    if chunk_id not in seen_chunks:
                        all_results.append(r)
                        seen_chunks.add(chunk_id)
        
        # 3. Para Search Fund, adicionar queries espec√≠ficas
        if section == "identification":
            for query in queries_config.get("search_fund", []):
                results = processor.search_relevant_chunks(query, embeddings_data, top_k=3)
                for r in results:
                    chunk_id = hash(r["chunk"][:100])
                    if chunk_id not in seen_chunks:
                        all_results.append(r)
                        seen_chunks.add(chunk_id)
        
        # Ordenar por score e pegar os melhores
        all_results.sort(key=lambda x: x["score"], reverse=True)
        
        # OTIMIZA√á√ÉO: identification e gestora costumam estar no in√≠cio do doc
        top_k = 15 if section in ("identification", "gestora") else 10
        final_results = all_results[:top_k]
        
        # Combinar chunks
        context = "\n\n".join([r["chunk"] for r in final_results])
        
        logger.info(
            f"üîç Busca hier√°rquica '{section}': "
            f"{len(final_results)} chunks √∫nicos de {len(all_results)} encontrados"
        )
        
        return context
    
    async def _get_relevant_context_smart(
        self,
        embeddings_data: Dict,
        section: str
    ) -> str:
        """
        Busca inteligente com suporte a ChromaDB OU NumPy usando queries hier√°rquicas.
        
        Detecta automaticamente se embeddings_data vem do ChromaDB ou NumPy:
        - ChromaDB: Usa query vetorial direta no index persistente
        - NumPy: Busca em mem√≥ria com metadata boost
        
        Usa queries hier√°rquicas: primeiro tenta queries prim√°rias (espec√≠ficas),
        depois expande com queries secund√°rias se necess√°rio.
        
        Args:
            embeddings_data: Dados com embeddings (ChromaDB ou NumPy)
            section: Nome da se√ß√£o a extrair
            
        Returns:
            Contexto relevante concatenado
        """
        from core.document_processor import DocumentProcessor
        processor = DocumentProcessor()
        
        # Obter queries hier√°rquicas
        queries_config = self._get_section_queries_hierarchical(section)
        
        # OTIMIZA√á√ÉO: identification e gestora costumam estar no in√≠cio do doc
        top_k = 15 if section in ("identification", "gestora") else 10
        
        # DETECTAR MODO: ChromaDB ou NumPy
        if embeddings_data.get("vector_store") == "chromadb":
            # MODO CHROMADB: Busca persistente com queries hier√°rquicas
            memo_id = embeddings_data["memo_id"]
            
            logger.info(f"üéØ Busca ChromaDB hier√°rquica para '{section}' (memo: {memo_id})")
            
            # Buscar com queries prim√°rias primeiro (mais espec√≠ficas)
            all_results = []
            seen_chunks = set()
            
            # 1. Buscar com queries prim√°rias (prioridade alta)
            for query in queries_config.get("primary", []):
                results = processor.search_chromadb_chunks(
                    memo_id=memo_id,
                    query=query,
                    top_k=5,
                    section=None
                )
                for r in results:
                    chunk_id = hash(r["chunk"][:100])  # ID √∫nico baseado no in√≠cio do chunk
                    if chunk_id not in seen_chunks:
                        all_results.append(r)
                        seen_chunks.add(chunk_id)
            
            # 2. Se n√£o encontrou o suficiente, buscar com queries secund√°rias
            if len(all_results) < 8:
                for query in queries_config.get("secondary", []):
                    results = processor.search_chromadb_chunks(
                        memo_id=memo_id,
                        query=query,
                        top_k=3,
                        section=None
                    )
                    for r in results:
                        chunk_id = hash(r["chunk"][:100])
                        if chunk_id not in seen_chunks:
                            all_results.append(r)
                            seen_chunks.add(chunk_id)
            
            # 3. Para Search Fund, adicionar queries espec√≠ficas
            if section == "identification":
                for query in queries_config.get("search_fund", []):
                    results = processor.search_chromadb_chunks(
                        memo_id=memo_id,
                        query=query,
                        top_k=3,
                        section=None
                    )
                    for r in results:
                        chunk_id = hash(r["chunk"][:100])
                        if chunk_id not in seen_chunks:
                            all_results.append(r)
                            seen_chunks.add(chunk_id)
            
            if not all_results:
                logger.warning(
                    f"‚ö†Ô∏è Busca ChromaDB hier√°rquica n√£o retornou resultados para '{section}'. "
                    f"Fallback para busca legado."
                )
                return await self._get_relevant_context(embeddings_data, section)
            
            # Ordenar por score e pegar os melhores
            all_results.sort(key=lambda x: x["score"], reverse=True)
            final_results = all_results[:top_k]
            
            # Combinar chunks
            context = "\n\n".join([r["chunk"] for r in final_results])
            
            # Log estat√≠sticas
            avg_score = sum(r["score"] for r in final_results) / len(final_results) if final_results else 0
            sections_found = set(
                r.get("metadata", {}).get("section_title", "Unknown")
                for r in final_results
            )
            
            logger.info(
                f"‚úÖ Busca ChromaDB hier√°rquica '{section}': "
                f"{len(final_results)} chunks √∫nicos (de {len(all_results)} encontrados) | "
                f"score m√©dio: {avg_score:.3f} | "
                f"se√ß√µes: {', '.join(list(sections_found)[:3])}"
            )
            
            return context
        
        else:
            # MODO NUMPY: Busca em mem√≥ria (fallback) com queries hier√°rquicas
            # Verificar se tem metadata
            if "metadata" not in embeddings_data:
                logger.warning(
                    f"‚ö†Ô∏è embeddings_data sem metadata. "
                    f"Fallback para busca legado."
                )
                return await self._get_relevant_context(embeddings_data, section)
            
            # Buscar com queries hier√°rquicas
            all_results = []
            seen_chunks = set()
            
            # 1. Buscar com queries prim√°rias (prioridade alta)
            for query in queries_config.get("primary", []):
                results = processor.search_relevant_chunks_with_metadata(
                    query=query,
                    embeddings_data=embeddings_data,
                    section_filter=None,
                    top_k=5
                )
                for r in results:
                    chunk_id = hash(r["chunk"][:100])
                    if chunk_id not in seen_chunks:
                        all_results.append(r)
                        seen_chunks.add(chunk_id)
            
            # 2. Se n√£o encontrou o suficiente, buscar com queries secund√°rias
            if len(all_results) < 8:
                for query in queries_config.get("secondary", []):
                    results = processor.search_relevant_chunks_with_metadata(
                        query=query,
                        embeddings_data=embeddings_data,
                        section_filter=None,
                        top_k=3
                    )
                    for r in results:
                        chunk_id = hash(r["chunk"][:100])
                        if chunk_id not in seen_chunks:
                            all_results.append(r)
                            seen_chunks.add(chunk_id)
            
            # 3. Para Search Fund, adicionar queries espec√≠ficas
            if section == "identification":
                for query in queries_config.get("search_fund", []):
                    results = processor.search_relevant_chunks_with_metadata(
                        query=query,
                        embeddings_data=embeddings_data,
                        section_filter=None,
                        top_k=3
                    )
                    for r in results:
                        chunk_id = hash(r["chunk"][:100])
                        if chunk_id not in seen_chunks:
                            all_results.append(r)
                            seen_chunks.add(chunk_id)
            
            if not all_results:
                logger.warning(
                    f"‚ö†Ô∏è Busca smart hier√°rquica n√£o retornou resultados para '{section}'. "
                    f"Tentando busca legado..."
                )
                return await self._get_relevant_context(embeddings_data, section)
            
            # Ordenar por score e pegar os melhores
            all_results.sort(key=lambda x: x.get("boosted_score", x["score"]), reverse=True)
            final_results = all_results[:top_k]
            
            # Log estat√≠sticas
            avg_score = sum(r["score"] for r in final_results) / len(final_results) if final_results else 0
            sections_found = set(
                r.get("metadata", {}).get("section_title", "Unknown")
                for r in final_results
            )
            
            logger.info(
                f"üéØ Busca sem√¢ntica hier√°rquica para '{section}': "
                f"{len(final_results)} chunks √∫nicos (de {len(all_results)} encontrados) | "
                f"score m√©dio: {avg_score:.3f} | "
                f"se√ß√µes encontradas: {', '.join(list(sections_found)[:3])}"
            )
        
            # Combinar chunks (menos contexto, mais preciso)
            context = "\n\n".join([r["chunk"] for r in final_results])
        
        # Log economia de tokens
        old_method_chars = 60000  # M√©todo antigo pegava ~60k chars
        new_method_chars = len(context)
        savings_pct = ((old_method_chars - new_method_chars) / old_method_chars) * 100
        
        logger.info(
            f"üí∞ Economia de tokens: {new_method_chars} chars "
            f"(vs {old_method_chars} antigo) = {savings_pct:.1f}% redu√ß√£o"
        )
        
        return context
    
    def _get_relevant_fields(self, memo_type: str) -> Dict:
        """
        Retorna apenas os campos relevantes para o tipo de memorando.

        Usa get_fatos_config(memo_type).get_relevant_fields_for_memo_type().
        """
        config = get_fatos_config(memo_type)
        all_relevant = config.get_relevant_fields_for_memo_type(memo_type)
        
        # Filtrar apenas os campos da se√ß√£o atual
        if self.section_name not in all_relevant:
            logger.warning(f"Se√ß√£o '{self.section_name}' n√£o tem campos relevantes para '{memo_type}'")
            return {}
        
        section_fields = all_relevant[self.section_name]
        
        # Criar dict com labels amig√°veis
        relevant = {}
        for field_key in section_fields:
            # Converter field_key em label (ex: "gestora_nome" ‚Üí "Gestora Nome")
            label = field_key.replace("_", " ").title()
            relevant[field_key] = label
        
        logger.info(f"Se√ß√£o '{self.section_name}' para '{memo_type}': {len(relevant)} campos relevantes")
        
        return relevant



