"""
Classe Base Comum para todos os Orchestrators LangGraph de Short Memo

Centraliza l√≥gica compartilhada entre:
- short_searchfund, short_gestora, short_primario
"""

from typing import TypedDict, Dict, Any, Optional, List, Literal
from langgraph.graph import StateGraph, END
from model_config import get_llm_for_agents
from core.logger import get_logger

logger = get_logger(__name__)


class BaseShortMemoGenerationState(TypedDict, total=False):
    """Estado compartilhado entre n√≥s do grafo LangGraph (base comum)"""
    # Input
    section_title: str
    agent: Any  # Agente especializado
    facts: Dict[str, Any]
    memo_id: Optional[str]
    processor: Optional[Any]  # DocumentProcessor
    
    # RAG Context
    section_rag_context: Optional[str]
    query: str
    
    # Generation
    generated_text: str
    paragraphs: List[str]
    
    # Validation
    validation_errors: List[str]
    retry_count: int
    max_retries: int
    
    # Final
    is_complete: bool
    final_output: Dict[str, List[str]]


class BaseLangGraphOrchestrator:
    """
    Classe base para todos os orchestrators de Short Memo.
    
    Implementa l√≥gica comum:
    - Gerenciamento de LLM compartilhado
    - Constru√ß√£o do grafo LangGraph
    - N√≥s comuns (_prepare_section, _generate_with_agent, etc)
    - Fluxo padr√£o com retry
    
    Subclasses devem:
    1. Definir fixed_structure
    2. Passar section_queries no __init__ se precisarem de queries espec√≠ficas
    3. Herdar tudo mais automaticamente
    """
    
    def __init__(
        self,
        fixed_structure: Dict[str, Any],
        section_queries: Optional[Dict[str, str]] = None,
        model: str = "gpt-4o",
        temperature: float = 0.25,
        max_retries: int = 2
    ):
        """
        Inicializa orchestrator base.
        
        Args:
            fixed_structure: Dict com estrutura fixa de se√ß√µes
            section_queries: Dict opcional de queries RAG espec√≠ficas por se√ß√£o
            model: Modelo OpenAI
            temperature: Criatividade (0-1)
            max_retries: Tentativas de retry se falhar
        """
        self.fixed_structure = fixed_structure
        self.section_queries = section_queries or {}
        self.model = model
        self.temperature = temperature
        self.max_retries = max_retries
        
        # LLM centralizado em model_config (OpenAI/Anthropic conforme cadastro)
        self.llm = get_llm_for_agents(model, temperature)
        
        # Construir grafo
        self.graph = self._build_graph()
    
    def _build_graph(self) -> StateGraph:
        """Constr√≥i o grafo LangGraph (implementa√ß√£o base comum)"""
        
        workflow = StateGraph(BaseShortMemoGenerationState)
        
        # === N√ìS COMUNS ===
        workflow.add_node("prepare_section", self._prepare_section)
        workflow.add_node("generate_with_agent", self._generate_with_agent)
        workflow.add_node("validate_output", self._validate_output)
        workflow.add_node("retry_section", self._retry_section)
        workflow.add_node("finalize", self._finalize)
        
        # === FLUXO ===
        workflow.set_entry_point("prepare_section")
        workflow.add_edge("prepare_section", "generate_with_agent")
        workflow.add_edge("generate_with_agent", "validate_output")
        
        workflow.add_conditional_edges(
            "validate_output",
            self._should_retry,
            {"retry": "retry_section", "finalize": "finalize", "end": END}
        )
        
        workflow.add_edge("retry_section", "generate_with_agent")
        workflow.add_edge("finalize", END)
        
        return workflow.compile()
    
    def _prepare_section(self, state: BaseShortMemoGenerationState) -> Dict[str, Any]:
        """N√≥ 1: Busca contexto RAG relevante no ChromaDB"""
        section_title = state["section_title"]
        memo_id = state.get("memo_id")
        processor = state.get("processor")
        
        logger.info(f"üîç [LangGraph] Preparando se√ß√£o '{section_title}'...")
        
        section_rag_context = None
        # Usar query espec√≠fica se dispon√≠vel, sen√£o usar t√≠tulo gen√©rico
        query = self.section_queries.get(section_title, section_title.lower().replace(" ", " "))
        
        if memo_id and processor:
            try:
                chunks = processor.search_chromadb_chunks(
                    memo_id=memo_id,
                    query=query,
                    top_k=10
                )
                
                if chunks:
                    section_rag_context = "\n\n".join([chunk["chunk"] for chunk in chunks])
                    logger.info(f"‚úÖ [LangGraph] {len(chunks)} chunks para '{section_title}'")
                else:
                    logger.warning(f"‚ö†Ô∏è [LangGraph] Nenhum chunk para '{section_title}'")
                    
            except Exception as e:
                logger.error(f"‚ùå [LangGraph] Erro RAG: {e}")
                section_rag_context = None
        else:
            logger.info(f"‚ÑπÔ∏è [LangGraph] Sem memo_id/processor, pulando RAG")
        
        return {
            "section_rag_context": section_rag_context,
            "query": query
        }
    
    def _generate_with_agent(self, state: BaseShortMemoGenerationState) -> Dict[str, Any]:
        """N√≥ 2: Chama agente especializado para gerar texto"""
        section_title = state["section_title"]
        agent = state["agent"]
        facts = state["facts"]
        section_rag_context = state.get("section_rag_context")
        
        logger.info(f"ü§ñ [LangGraph] Gerando '{section_title}' com {agent.__class__.__name__}...")
        
        try:
            # Injetar LLM compartilhado no agente (se suportar)
            if hasattr(agent, 'set_llm'):
                agent.set_llm(self.llm)
            
            # Chamar m√©todo generate do agente
            generated_text = agent.generate(
                facts=facts,
                rag_context=section_rag_context
            )
            
            logger.info(f"‚úÖ [LangGraph] Texto gerado ({len(generated_text)} chars)")
            
            return {"generated_text": generated_text}
            
        except Exception as e:
            logger.error(f"‚ùå [LangGraph] Erro ao gerar: {e}")
            return {
                "generated_text": f"(Erro ao gerar se√ß√£o: {e})",
                "validation_errors": [f"Erro na gera√ß√£o: {str(e)}"]
            }
    
    def _validate_output(self, state: BaseShortMemoGenerationState) -> Dict[str, Any]:
        """N√≥ 3: Valida qualidade e formato do texto gerado"""
        section_title = state["section_title"]
        generated_text = state.get("generated_text", "")
        
        logger.info(f"üîç [LangGraph] Validando output...")
        
        errors = []
        
        # Valida√ß√£o 1: Texto n√£o vazio
        if not generated_text or not generated_text.strip():
            errors.append("Texto gerado est√° vazio")
        
        # Valida√ß√£o 2: M√≠nimo de caracteres
        if len(generated_text.strip()) < 100:
            errors.append(f"Texto muito curto ({len(generated_text)} chars, m√≠n: 100)")
        
        # Valida√ß√£o 3: Verificar placeholder de erro
        if "(Erro" in generated_text or "Erro ao gerar" in generated_text:
            errors.append("Texto cont√©m placeholder de erro")
        
        # Score de qualidade
        quality_score = 0.9 if not errors else max(0.5, 1.0 - len(errors) * 0.2)
        
        if errors:
            logger.warning(f"‚ö†Ô∏è [LangGraph] {len(errors)} erro(s): {errors}")
        else:
            logger.info(f"‚úÖ [LangGraph] Valida√ß√£o OK (score: {quality_score})")
        
        return {
            "validation_errors": errors,
            "quality_score": quality_score
        }
    
    def _should_retry(self, state: BaseShortMemoGenerationState) -> Literal["retry", "finalize", "end"]:
        """Decis√£o se deve fazer retry"""
        retry_count = state.get("retry_count", 0)
        max_retries = state.get("max_retries", self.max_retries)
        errors = state.get("validation_errors", [])
        
        if errors and retry_count < max_retries:
            logger.info(f"üîÑ [LangGraph] Retry {retry_count + 1}/{max_retries}")
            return "retry"
        
        return "finalize"
    
    def _retry_section(self, state: BaseShortMemoGenerationState) -> Dict[str, Any]:
        """N√≥ 4: Incrementa contador de retry"""
        new_count = state.get("retry_count", 0) + 1
        logger.info(f"üîÑ [LangGraph] Tentativa {new_count}")
        return {"retry_count": new_count}
    
    def _finalize(self, state: BaseShortMemoGenerationState) -> Dict[str, Any]:
        """N√≥ 5: Finaliza e formata resultado"""
        generated_text = state.get("generated_text", "")
        
        # Quebrar em par√°grafos
        paragraphs = [p.strip() for p in generated_text.split("\n\n") if p.strip()]
        
        logger.info(f"‚úÖ [LangGraph] Finalizado: {len(paragraphs)} par√°grafos")
        
        return {
            "is_complete": True,
            "paragraphs": paragraphs,
            "final_output": {"text": paragraphs}
        }
    
    def generate_full_memo(
        self,
        facts: Dict[str, Any],
        memo_id: Optional[str] = None,
        processor: Optional[Any] = None,
        rag_context: Optional[str] = None  # Deprecated
    ) -> Dict[str, List[str]]:
        """
        Gera memo completo orquestrando todos os agentes via LangGraph.
        
        Args:
            facts: Facts estruturados (todas as se√ß√µes)
            memo_id: ID do memo no ChromaDB para RAG
            processor: DocumentProcessor para busca RAG
            rag_context: DEPRECATED - ignorado
            
        Returns:
            Dict com estrutura: {section_title: [paragraph1, paragraph2, ...]}
        """
        if rag_context:
            logger.warning("‚ö†Ô∏è rag_context deprecated, use memo_id+processor")
        
        result = {}
        
        for section_title, agent in self.fixed_structure.items():
            logger.info(f"\n{'='*60}")
            logger.info(f"üìù Processando: {section_title}")
            logger.info(f"{'='*60}\n")
            
            # Estado inicial para esta se√ß√£o
            initial_state = {
                "section_title": section_title,
                "agent": agent,
                "facts": facts,
                "memo_id": memo_id,
                "processor": processor,
                "retry_count": 0,
                "max_retries": self.max_retries
            }
            
            # Executar grafo (sincronamente)
            try:
                # O grafo √© compilado, ent√£o executamos como fun√ß√£o
                final_state = self.graph.invoke(initial_state)
                paragraphs = final_state.get("paragraphs", [])
                result[section_title] = paragraphs
                
            except Exception as e:
                logger.error(f"‚ùå Erro ao processar '{section_title}': {e}")
                result[section_title] = [f"(Erro ao gerar se√ß√£o: {e})"]
        
        logger.info(f"\n‚úÖ Memo gerado com {len(result)} se√ß√µes")
        return result
